{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "This Python notebook contains all the necessary and sufficient code to reproduce the results that were obtained for the original article. The resulting files are all saved and available on github.\n",
        "\n",
        "1. [Data pre-processing](#preprocessing)\n",
        "2. [PCA and LR](#pca-and-lr)\n",
        "3. [PCA and SVM](#pca-and-svm)\n",
        "3. [PCA and RF](#pca-and-rf)\n",
        "4. [PCA and MLP](#pca-and-mlp)\n",
        "5. [Probability map (biomarker) for AD](#probability-map)"
      ],
      "metadata": {
        "id": "j6k9v75rvUvb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B--_9cqMKlq8"
      },
      "source": [
        "## Data pre-processing <a name=\"preprocessing\"></a>\n",
        "\n",
        "All cells below contain the code related to pre-processing the neuroimages before being used for training/testing the classification models. The function below, for example, is responsible for loading neuroimages as *numpy arrays*, flatten or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzm3BDRnR8qK"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# load neuroimages from specified path: returns an array containing all the images\n",
        "# from the given path. The output can be flatten or not\n",
        "def load_imgs_from_path(path, vectorize = True):\n",
        "\n",
        "    # listing img files\n",
        "    all_imgs_list = os.listdir(path)\n",
        "    all_imgs_list = [file for file in all_imgs_list if file[-3:] == \"img\"]\n",
        "\n",
        "    # loading imgs\n",
        "    all_imgs = []\n",
        "    for img_file in all_imgs_list:\n",
        "        all_imgs.append(nib.load(path + img_file).get_fdata())\n",
        "\n",
        "    # returning final list as an array (vectorized or not)\n",
        "    if vectorize:\n",
        "        all_imgs = np.array(all_imgs)\n",
        "        return all_imgs.reshape((all_imgs.shape[0], np.prod(all_imgs.shape[1:])))\n",
        "    else:\n",
        "        return np.array(all_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A377_B7UKtIK"
      },
      "source": [
        "To load the neuroimages, simply use the function above over the appropriate directories. Next, the neuroimages must be masked and passed through the [`scale`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOV1ZP5ADrxg"
      },
      "outputs": [],
      "source": [
        "from sklearn.pre_processing import scale\n",
        "\n",
        "# loading ADNI data\n",
        "ADNI_ad = load_imgs_from_path(\"./ADNI_AD/\")\n",
        "ADNI_cn = load_imgs_from_path(\"./ADNI_CN/\")\n",
        "\n",
        "# loading CDI data\n",
        "clinic_ad = load_imgs_from_path(\"./CDI_AD/\")\n",
        "clinic_nad = load_imgs_from_path(\"./CDI_nAD/\")\n",
        "\n",
        "# loading mask\n",
        "mask = load_imgs_from_path(\"./mask/\").astype(int)\n",
        "# flattening mask\n",
        "mask = mask.reshape((mask.size, ))\n",
        "\n",
        "## concatenating adni and CDI data separately\n",
        "# and then masking and scaling it\n",
        "\n",
        "# ADNI data\n",
        "adni_data = np.concatenate((ADNI_ad, ADNI_cn))\n",
        "adni_dataM = adni_data[:, mask == 1]\n",
        "adni_dataMS = scale(adni_dataM, axis = 1)\n",
        "\n",
        "# clinic data\n",
        "clinic_data = np.concatenate((clinic_ad, clinic_nad))\n",
        "clinic_dataM = clinic_data[:, mask == 1]\n",
        "clinic_dataMS = scale(clinic_dataM, axis = 1)\n",
        "\n",
        "# generating respective labels (AD as 1)\n",
        "adni_labels = [1]*100 + [0]*100\n",
        "clinic_labels = [1]*92 + [0]*100\n",
        "\n",
        "# deleting some of the variables so we can use less RAM\n",
        "del ADNI_ad, ADNI_cn, clinic_ad, clinic_nad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdsSKxHsNI8Z"
      },
      "source": [
        "Organizing and saving training and testing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkZhBXRdNcNK"
      },
      "outputs": [],
      "source": [
        "import joblib as jb\n",
        "\n",
        "# training data\n",
        "X_train = adni_dataMS\n",
        "y_train = adni_labels\n",
        "jb.dump((X_train, y_train), \"training_data.pkl\")\n",
        "\n",
        "# testing data\n",
        "X_test = clinic_dataMS\n",
        "y_test = clinic_labels\n",
        "jb.dump((X_test, y_test), \"testing_data.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA and LR <a name=\"pca-and-lr\"></a>\n",
        "\n",
        "All code related to PCA and LR modeling is shown below. The `GridSearchCV` object was saved so it could be used later."
      ],
      "metadata": {
        "id": "ZRIMzA27cYfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iowPcx4N-WJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib as jb\n",
        "\n",
        "# loading training data\n",
        "X_train, y_train = jb.load(\"training_data.pkl\")\n",
        "\n",
        "# loading testing data\n",
        "X_test, y_test = jb.load(\"testing_data.pkl\")\n",
        "\n",
        "# pipeline with PCA and Logistic Regression\n",
        "pipe = Pipeline([(\"PCA\", PCA(n_components = .8, whiten = True)),\n",
        "                 (\"LR\", LogisticRegression(solver = \"saga\", n_jobs = -1, max_iter = 5000))])\n",
        "\n",
        "# param_grid for GridSearch\n",
        "param_grid = [{\"LR__penalty\": [\"l1\", \"l2\"],\n",
        "              \"LR__C\": np.logspace(-2, 2, 25)},\n",
        "              {\"LR__penalty\": [\"none\"]}]\n",
        "\n",
        "# fitting GridSearch and saving the results\n",
        "gs = GridSearchCV(pipe, param_grid, scoring = [\"accuracy\", \"recall\", \"precision\", \"f1\"], refit = \"f1\")\n",
        "gs.fit(X_train, y_train)\n",
        "jb.dump(gs, \"gs_fitted_LR.pkl\")\n",
        "\n",
        "# saving gs results as a spreadsheet\n",
        "pd.DataFrame(gs.cv_results_).to_excel(\"params_tbl_LR.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA and SVM <a name=\"pca-and-svm\"></a>\n",
        "\n",
        "All code related to PCA and SVM modeling is shown below. The `GridSearchCV` object was saved so it could be used later."
      ],
      "metadata": {
        "id": "d3Jzjm2Rpf64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib as jb\n",
        "\n",
        "# loading training data\n",
        "X_train, y_train = jb.load(\"training_data.pkl\")\n",
        "\n",
        "# loading testing data\n",
        "X_test, y_test = jb.load(\"testing_data.pkl\")\n",
        "\n",
        "# pipeline with PCA and SVC\n",
        "pipe = Pipeline([(\"PCA\", PCA(n_components = .8, whiten = True)),\n",
        "                 (\"SVC\", SVC(probability=True))])\n",
        "\n",
        "# param_grid for GridSearch\n",
        "param_grid = [{\"SVC__kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
        "             \"SVC__C\": np.logspace(-3, 2, 8),\n",
        "             \"SVC__gamma\": np.logspace(-3, 2, 8)},\n",
        "             {\"SVC__kernel\": [\"linear\"],\n",
        "             \"SVC__C\": np.logspace(-3, 2, 8)}]\n",
        "\n",
        "# fitting GridSearch and saving the results\n",
        "gs = GridSearchCV(pipe, param_grid, scoring = [\"accuracy\", \"recall\", \"precision\", \"f1\"], refit = \"f1\")\n",
        "gs.fit(X_train, y_train)\n",
        "jb.dump(gs, \"gs_fitted_SVC.pkl\")\n",
        "\n",
        "# saving gs results as a spreadsheet\n",
        "pd.DataFrame(gs.cv_results_).to_excel(\"params_tbl_SVC.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "5VV1tuCbpylI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA and RF <a name=\"pca-and-rf\"></a>\n",
        "\n",
        "All code related to PCA and RF modeling is shown below. The `GridSearchCV` object was saved so it could be used later."
      ],
      "metadata": {
        "id": "LjmUJos6qJDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib as jb\n",
        "\n",
        "# loading training data\n",
        "X_train, y_train = jb.load(\"training_data.pkl\")\n",
        "\n",
        "# loading testing data\n",
        "X_test, y_test = jb.load(\"testing_data.pkl\")\n",
        "\n",
        "# pipeline with PCA and Random Forest\n",
        "pipe = Pipeline([(\"PCA\", PCA(n_components = .8, whiten = True)),\n",
        "                 (\"RF\", RandomForestClassifier(n_jobs = -1))])\n",
        "\n",
        "# param_grid for GridSearch\n",
        "param_grid = {\"RF__n_estimators\": [50, 100, 150],\n",
        "             \"RF__max_depth\": list(range(3,10)) + [\"None\"],\n",
        "             \"RF__max_features\": range(5, 16)}\n",
        "\n",
        "# fitting GridSearch and saving the results\n",
        "gs = GridSearchCV(pipe, param_grid, scoring = [\"accuracy\", \"recall\", \"precision\", \"f1\"], refit = \"f1\")\n",
        "gs.fit(X_train, y_train)\n",
        "jb.dump(gs, \"gs_fitted_RF.pkl\")\n",
        "\n",
        "# saving gs results as a spreadsheet\n",
        "pd.DataFrame(gs.cv_results_).to_excel(\"params_tbl_RF.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "AjVTgW6iqQjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA and MLP <a name=\"pca-and-mlp\"></a>\n",
        "\n",
        "All code related to PCA and MLP modeling is shown below. The `GridSearchCV` object was saved so it could be used later."
      ],
      "metadata": {
        "id": "xOlrZq1sqrHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib as jb\n",
        "\n",
        "# pipeline with PCA and Multilayer Perceptron\n",
        "pipe = Pipeline([(\"PCA\", PCA(n_components = .8, whiten = True)),\n",
        "                 (\"MLP\", MLPClassifier())])\n",
        "\n",
        "# param_grid for GridSearch\n",
        "param_grid = {\"MLP__alpha\": np.logspace(-4, 2, 30),\n",
        "             \"MLP__hidden_layer_sizes\": [(80,), (80, 70), (80, 70, 60)],\n",
        "             \"MLP__activation\": [\"logistic\", \"tanh\", \"relu\"]}\n",
        "\n",
        "# fitting GridSearch and saving the results\n",
        "gs = GridSearchCV(pipe, param_grid, scoring = [\"accuracy\", \"recall\", \"precision\", \"f1\"], refit = \"f1\")\n",
        "gs.fit(X_train, y_train)\n",
        "jb.dump(gs, \"gs_fitted_MLP.pkl\")\n",
        "\n",
        "# saving gs results as a spreadsheet\n",
        "pd.DataFrame(gs.cv_results_).to_excel(\"params_tbl_MLP.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "Z_sggzzoqzxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y1BjA7OyiP_"
      },
      "source": [
        "## Probability map (biomarker) for AD <a name=\"probability-map\"></a>\n",
        "\n",
        "The code below refers to calculating the probability map (or biomarker) for Alzheimer's disease. The resulting .img/.hdr file can easily be opened using SPM or MRIcroGL for further visual processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y5duVQFvYlZ"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import joblib as jb\n",
        "import numpy as np\n",
        "\n",
        "# loading mask\n",
        "mask = load_imgs_from_path(\"./mask/\").astype(int)\n",
        "# flattening mask\n",
        "mask = mask.reshape((mask.size, ))\n",
        "\n",
        "# loading gs_LR\n",
        "gs = jb.load(\"./gs_fitted_LR.pkl\")\n",
        "# separating classifier\n",
        "clf = gs.best_estimator_[\"LR\"]\n",
        "# separating pca\n",
        "pca = gs.best_estimator_[\"PCA\"]\n",
        "\n",
        "# initializing variable using the mask\n",
        "biomarker = mask.copy().astype(float)\n",
        "\n",
        "# for values ​​other than 0, enter the product between the principal components\n",
        "# and their respective weights of the logistic regression model\n",
        "biomarker[biomarker != 0] = np.sum(pca.components_ * clf.coef_.T, axis = 0)\n",
        "\n",
        "# reshaping so that data can be saved using nib\n",
        "biomarker = biomarker.reshape((79, 95, 79, 1))\n",
        "\n",
        "# loading mask_file using nib so we can use affine and header attributes\n",
        "# for the biomarker data (it could be any neuroimage in place of the mask)\n",
        "mask_file = nib.load(\"./mask/OKmask20.img\")\n",
        "biomarker = nib.Nifti1Image(biomarker, affine = mask_file.affine, header = mask_file.header)\n",
        "\n",
        "# saving object as .img/.hdr file for further image processing\n",
        "nib.save(biomarker, \"biomarker.img\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}